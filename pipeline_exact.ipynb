{
 "cells": [
  {
   "cell_type": "code",

   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline as skPipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from CustomTransformers import StringCleanTransformer, DayTimeTransformer, DropperTransformer, SignTransformer, OHTransformer, FloatTransformer, biningTransformer, weekdayTransformer, TotalTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 25,

   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/training.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "train =train.sort_values(by=['CustomerId', \"TransactionStartTime\"])\n",
    "train_Y = train.FraudResult\n",
    "train.drop(['FraudResult'], axis=1, inplace=True)\n",
    "\n",
    "test = test.sort_values(by=['CustomerId', \"TransactionStartTime\"])\n",
    "\n",
    "StringToClean = [\"TransactionId\", \"BatchId\",\"AccountId\",\"SubscriptionId\",\"CustomerId\", \"ProviderId\", \"ProductId\", \"ChannelId\", \"ProductCategory\"]\n",
    "\n",
    "drop_cols = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\"]\n",
    "hot_cols = [\"ProductCategory\"]\n",
    "bin_cols = [\"TransactionStartTime\"]\n",
    "smt  = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Must have at least 1 validation dataset for early stopping.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\engel\\Documents\\4MIN\\MachineLeanring\\Xente\\ML_Xente\\pipeline_exact.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m imbPipeline(steps \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mclean\u001b[39m\u001b[39m\"\u001b[39m, StringCleanTransformer()),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mamout to sign\u001b[39m\u001b[39m\"\u001b[39m, SignTransformer()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, XGBClassifier(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, n_estimators\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m preprocessor\u001b[39m.\u001b[39mfit(train\u001b[39m.\u001b[39mcopy(), train_Y\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train\u001b[39m.\u001b[39mhead()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/engel/Documents/4MIN/MachineLeanring/Xente/ML_Xente/pipeline_exact.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_res \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mpredict(test\u001b[39m.\u001b[39mcopy())\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\imblearn\\pipeline.py:297\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    296\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 297\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, yt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\training.py:186\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    189\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\callback.py:247\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    245\u001b[0m     metric_score \u001b[39m=\u001b[39m [(n, \u001b[39mfloat\u001b[39m(s)) \u001b[39mfor\u001b[39;00m n, s \u001b[39min\u001b[39;00m metric_score_str]\n\u001b[0;32m    246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 247\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39;49m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory)\n\u001b[0;32m    248\u001b[0m           \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks)\n\u001b[0;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\callback.py:247\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    245\u001b[0m     metric_score \u001b[39m=\u001b[39m [(n, \u001b[39mfloat\u001b[39m(s)) \u001b[39mfor\u001b[39;00m n, s \u001b[39min\u001b[39;00m metric_score_str]\n\u001b[0;32m    246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 247\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory)\n\u001b[0;32m    248\u001b[0m           \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[0;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\xgboost\\callback.py:412\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[1;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[0;32m    410\u001b[0m epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarting_round  \u001b[39m# training continuation\u001b[39;00m\n\u001b[0;32m    411\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 412\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(evals_log\u001b[39m.\u001b[39mkeys()) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, msg\n\u001b[0;32m    413\u001b[0m data_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata:\n",
      "\u001b[1;31mAssertionError\u001b[0m: Must have at least 1 validation dataset for early stopping."
     ]
    }
   ],
   "source": [
    "preprocessor = imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"total\", TotalTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    # (\"binning\", biningTransformer(bin_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier(learning_rate=0.5, n_estimators=600, max_depth=4, early_stopping_rounds=20))\n",
    "])\n",
    "\n",
    "preprocessor.fit(train.copy(), train_Y.copy())\n",
    "\n",
    "train.head()\n",
    "test_res = preprocessor.predict(test.copy())\n",
    "\n",
    "output = pd.DataFrame()\n",
    "output[\"TransactionId\"] = test[\"TransactionId\"]\n",
    "output[\"FraudResult\"] = test_res\n",
    "print(output.head(20))\n",
    "\n",
    "#save the result to csv file\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OH(X, hot_cols):\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    for elem in hot_cols:\n",
    "        OH_cols = pd.DataFrame(OH_encoder.fit_transform(X[elem].values.reshape(-1,1)))\n",
    "        OH_cols.rename(columns=lambda x: elem + str(x), inplace=True)\n",
    "        OH_cols.index = X.index\n",
    "        X = pd.concat([X, OH_cols], axis=1)\n",
    "    return X\n",
    "\n",
    "drop_cols_diff = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\", \"ProductCategory\"]\n",
    "\n",
    "pipelineXGB_diff= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols_diff)),\n",
    "    # (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# param_grid = {'max_depth': [2, 4, 6,10], 'n_estimators': [ 40,300,500,600], 'learning_rate': [ 0.3,0.4,0.5,0.6]}\n",
    "# param_grid = {'model__n_estimators': [10,100,500,600]}\n",
    "param_grid = {'model__n_estimators': [550,600,650,700,800], 'model__max_depth': [2,3,4,6], 'model__learning_rate': [ 0.5,0.55,0.575, 0.6]}\n",
    "\n",
    "if (False):\n",
    "    grid_search = GridSearchCV(pipelineXGB_diff, param_grid, scoring='f1')\n",
    "    grid_search.fit(OH(train, hot_cols), train_Y)\n",
    "\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "# {'model__learning_rate': 0.5, 'model__max_depth': 4, 'model__n_estimators': 500}\n",
    "# {'model__learning_rate': 0.55, 'model__max_depth': 3, 'model__n_estimators': 600}\n",
    "# {'model__learning_rate': 0.5, 'model__max_depth': 4, 'model__n_estimators': 600}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
