{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Machine Learning : Xente Fraud Detection\n",
    "Auteurs : Théo Engels, Hadrien Godts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline as skPipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from CustomTransformers import StringCleanTransformer, DayTimeTransformer, DropperTransformer, SignTransformer, OHTransformer, FloatTransformer, biningTransformer, weekdayTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des données\n",
    "train = pd.read_csv(\"data/training.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# shuffle et indication de la target\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "train_Y = train.FraudResult\n",
    "train.drop(['FraudResult'], axis=1, inplace=True)\n",
    "\n",
    "# features dont le string est à nettoyer\n",
    "StringToClean = [\"TransactionId\", \"BatchId\",\"AccountId\",\"SubscriptionId\",\"CustomerId\", \"ProviderId\", \"ProductId\", \"ChannelId\", \"ProductCategory\"]\n",
    "\n",
    "# features à dropper, one-hot-encoder et binariser respectivement, et initialisation du smote\n",
    "drop_cols = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\"]\n",
    "hot_cols = [\"ProductCategory\"]\n",
    "bin_cols = [\"TransactionStartTime\"]\n",
    "smt  = SMOTE()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Ce rapport présente les processus et les résultats de notre travail sur le Xente fraud detection challenge. Nous commençons par une analyse des données présentes dans le dataset, suivie de la phase de feature engineering. Nous présentons ensuite les différents modèles que nous avons entrainés et leurs performances, et nous terminons par une courte conclusion et quelques perspectives de recherche.\n",
    "\n",
    "L’objectif de ce challenge est de prédire si une transaction bancaire est frauduleuse ou pas, sur la base des différents features contenus dans le dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données\n",
    "\n",
    "La première chose importante à noter est que le training set contient 95 662 entrées, parmi lesquelles on compte 95 469 transactions légales et 193 frauduleuses (soit 0.2 %) : c’est un déséquilibre fort et important à noter, car le manque de transactions frauduleuses risque de compliquer l’entrainement des modèles. Nous pallions cette faiblesse dans le feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de transactions :  95662\n",
      "Nb de fraudes :  193\n",
      "Nb de non fraudes :  95469\n",
      "ratio fraudes/non frondes :  0.0020215986341115964\n"
     ]
    }
   ],
   "source": [
    "# Code pour nbr transactions\n",
    "print(\"Nb de transactions : \",train_Y.count())\n",
    "print(\"Nb de fraudes : \",train_Y.value_counts()[1])\n",
    "print(\"Nb de non fraudes : \",train_Y.value_counts()[0])\n",
    "print(\"ratio fraudes/non frondes : \",train_Y.value_counts()[1] / train_Y.value_counts()[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde chose importante à propos des données est qu’il ne manque aucune donnée : toutes les entrées ont des données pour chaque feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there missing values :  False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>BatchId</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>SubscriptionId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>ProviderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>ChannelId</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Value</th>\n",
       "      <th>TransactionStartTime</th>\n",
       "      <th>PricingStrategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_72536</td>\n",
       "      <td>BatchId_94324</td>\n",
       "      <td>AccountId_3432</td>\n",
       "      <td>SubscriptionId_841</td>\n",
       "      <td>CustomerId_3867</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_3</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2019-02-04T19:23:42Z</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_51068</td>\n",
       "      <td>BatchId_117017</td>\n",
       "      <td>AccountId_1653</td>\n",
       "      <td>SubscriptionId_443</td>\n",
       "      <td>CustomerId_2033</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_1</td>\n",
       "      <td>ProductId_15</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2019-01-16T14:44:23Z</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_18503</td>\n",
       "      <td>BatchId_57323</td>\n",
       "      <td>AccountId_4584</td>\n",
       "      <td>SubscriptionId_4030</td>\n",
       "      <td>CustomerId_5048</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_3</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019-02-10T10:48:40Z</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_140415</td>\n",
       "      <td>BatchId_43725</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_800</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-5000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2019-02-08T15:18:54Z</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_4562</td>\n",
       "      <td>BatchId_25375</td>\n",
       "      <td>AccountId_4249</td>\n",
       "      <td>SubscriptionId_4429</td>\n",
       "      <td>CustomerId_7343</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_3</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-30000.0</td>\n",
       "      <td>30000</td>\n",
       "      <td>2018-12-04T15:51:44Z</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
       "0   TransactionId_72536   BatchId_94324  AccountId_3432   SubscriptionId_841   \n",
       "1   TransactionId_51068  BatchId_117017  AccountId_1653   SubscriptionId_443   \n",
       "2   TransactionId_18503   BatchId_57323  AccountId_4584  SubscriptionId_4030   \n",
       "3  TransactionId_140415   BatchId_43725  AccountId_4841  SubscriptionId_3829   \n",
       "4    TransactionId_4562   BatchId_25375  AccountId_4249  SubscriptionId_4429   \n",
       "\n",
       "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
       "0  CustomerId_3867          UGX          256  ProviderId_6   ProductId_3   \n",
       "1  CustomerId_2033          UGX          256  ProviderId_1  ProductId_15   \n",
       "2  CustomerId_5048          UGX          256  ProviderId_6   ProductId_3   \n",
       "3   CustomerId_800          UGX          256  ProviderId_4   ProductId_6   \n",
       "4  CustomerId_7343          UGX          256  ProviderId_4   ProductId_3   \n",
       "\n",
       "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
       "0             airtime  ChannelId_3   1000.0   1000  2019-02-04T19:23:42Z   \n",
       "1  financial_services  ChannelId_3   5000.0   5000  2019-01-16T14:44:23Z   \n",
       "2             airtime  ChannelId_3   2000.0   2000  2019-02-10T10:48:40Z   \n",
       "3  financial_services  ChannelId_2  -5000.0   5000  2019-02-08T15:18:54Z   \n",
       "4             airtime  ChannelId_2 -30000.0  30000  2018-12-04T15:51:44Z   \n",
       "\n",
       "   PricingStrategy  \n",
       "0                2  \n",
       "1                2  \n",
       "2                2  \n",
       "3                2  \n",
       "4                4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code pour données manquantes\n",
    "\n",
    "print(\"Are there missing values : \", train.isnull().values.any())\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons ensuite les différents feature du dataset. Nous mentionnons ici les détails importants ainsi que les premières décisions prises concernant certaines features.\n",
    "-\tTransactionId : numéro de la transaction. Cette donnée est à garder, car elle permet de soumettre les résultats au défi, mais nous retirerons cette colonne du dataset dans la mesure où le numéro de la transaction ne devrait rien nous apprendre de particulier sur son caractère frauduleux.\n",
    "\n",
    "-\tBatchId : le numéro de groupe de transaction. Comme pour le numéro de transaction, cette donnée ne devrait rien nous apprendre sur le caractère frauduleux d’une transaction.\n",
    "\n",
    "-\tAccountId : l’identifiant d’un compte client. Nous conservons cette donnée.\n",
    "\n",
    "-\tSubscriptionId : l’identifiant d’une souscription. D’après la description des features, cette donnée semble liée à un compte client, sans autre explication particulièrement révélatrice. Vu ce lien et ce manque de clarté, nous décidons de laisser cette variable hors du dataset, afin d’en réduire la taille.\n",
    "-\tCustomerId : l’identifiant d’un client. À priori, un client peut disposer de plusieurs comptes, donc pour réduire la taille du dataset, nous laissons cette variable hors du dataset en privilégiant AccountId. Cela étant, si on peut établir qu’un compte fraude plus que les autres, il sera pertinent de reconstruire le lien vers le CustomerId.\n",
    "-\tCurrencyCode : la monnaie de la transaction. Il n’y a qu’une seule valeur pour ce feature, donc nous le laissons également hors du dataset.\n",
    "-\tCountryCode : le code du pays de la transaction. Idem que CurrencyCode, nous le laissons donc également hors du dataset.\n",
    "-\tProviderId : indique la source du produit acheté via la transaction. Dans un premier temps, nous conservons cette feature en faisant un one-hot encoding sur les six valeurs possibles, en partant du principe qu'il peut y avoir un lien entre la source du produit et le caractère frauduleux de la transaction.\n",
    "-\tProductId : donne le code du produit qui a été acheté lors de la transaction. Cette colonne contient 23 valeurs possibles, pour éviter de multiplier les features, nous  ne faisons dans un premier temps pas de oneHot encoding sur celle-ci. En outre, nous disposons de la variable ProductCategory (voir ci-après) donc nous décidons de laisser celle-ci hors du dataset. \n",
    "-\tProductCategory : indique la catégorie de produit qui a été fait l’objet de la transaction. Il y a 9 catégories possibles, donc nous pouvons faire du one-hot encoding pour conserver cette feature.\n",
    "-\tChannelId : indique la méthode de paiement utilisée pour effectuer la transaction. Cette feature nous semble importante, nous la conservons donc en faisant du one-hot encoding une fois de plus.\n",
    "-\tAmount : indique le montant de la transaction, avec son signe (+ ou - pour crédit ou débit). La feature value nous donne déjà le montant absolu, nous faisons donc d'abord du binary encoding pour que les montants positifs soient désormais true, et les montants négatifs false.\n",
    "-\tValue : indique le montant de la transaction, sans son signe. Nous conservons cette feature.\n",
    "-\tTransactionStartTime : indique la date et l'horaire de la transaction. Nous convertissons le format de cette donnée pour avoir un ordre chronologique, puis nous la séparons en une colonne date et une colonne heure. La colonne date est convertie pour avoir le jour de la semaine. La colonne heure laissée telle quelle pour le moment.\n",
    "-\tPricingStrategy : indique la structure de prix de Xente pour ses différents marchands. Sans certitude sur ce que cette colonne représente exactement, nous la one-hot encodons et verrons par la suite si elle a un impact ou non.\n",
    "-\tFraudResult : indique si la transaction est frauduleuse. Il s’agit de notre target, nous la gardons donc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nunique :\n",
      " TransactionId           95662\n",
      "BatchId                 94809\n",
      "AccountId                3633\n",
      "SubscriptionId           3627\n",
      "CustomerId               3742\n",
      "CurrencyCode                1\n",
      "CountryCode                 1\n",
      "ProviderId                  6\n",
      "ProductId                  23\n",
      "ProductCategory             9\n",
      "ChannelId                   4\n",
      "Amount                   1676\n",
      "Value                    1517\n",
      "TransactionStartTime    94556\n",
      "PricingStrategy             4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Code pour nunique\n",
    "\n",
    "print(\"nunique :\\n\", train.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Le dataset est déjà séparé en training et en validation, ce qui nous épargne cette étape, et nous l'avons déjà mélangé au moment de la lecture. Afin d’organiser efficacement notre travail, nous rédigeons le code sous forme de pipelines.\n",
    "\n",
    "Nous créons un préprocesseur pour la transformation des données que nous appliquons au training set. Les transformations du premier préprocesseur sont décrites dans le tableau suivant :\n",
    "\n",
    "| Transformateur | Description |\n",
    "| --- | --- |\n",
    "| clean | Pour les colonnes dont les données sont précédées d'un string (par exemple : BatchID578), retire le texte et ne conserve que les données numériques |\n",
    "| amount_to_sign | Remplace les valeurs de la colonne amount par true si elles sont positives et false si elles sont négatives ; la colonne est renommée « sign ». |\n",
    "| day_time_separator | Convertis le TransactionStartTime en format yyyymmddhhmmss, et sépare la colonne en jour (TransactionStartDay) et en heure (TransactionStartTime). |\n",
    "| dropper | Supprime les features non-pertinentes, qui sont listées dans la liste \"drop-cols\"|\n",
    "| one_hot_encode | Effectue un one-hot-encoding pour les colonnes de la list \"hot_cols\"|\n",
    "| weekday | Convertis la colonne TransactionStartDay en jour de la semaine (représenté par un entier allant de 1 à 7) plutôt qu'en date  |\n",
    "| float | Impose le type float sur toutes les colonnes (certaines colonnes du one-hot encoding ne sont plus en float). De plus, il passe les transactionId (dont nous avons besoin pour la soumission) en index. |\n",
    "| smote | Applique un suréchantillonnage artificiel sur le dataset afin d'équilibrer le dataset. |\n",
    "\n",
    "Notez que le smote sera ignoré lors du predict(test)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour le pipeline\n",
    "\n",
    "pipeline = imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier(n_estimators = 500))\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure de performances\n",
    "Afin d'estimer la performance de notre modèle, il est possible de séparer notre dataset en un set d'entrainement et un set de validation. Le set de validation permet de mesurer le score \"f1\" du modèle, cependant le score calculé par cette méthode est bien plus élevé que le score calculé sur le site du challenge. Ce problème peut potentiellement venir d'une fuite de données lors de notre feature engineering, cependant la source exacte n'a pas été trouvée.\n",
    "Cette méthode d'évaluation a donc été abandonnée.\n",
    "\n",
    "| Modèle | F1 score crossvalidation | Public Score | Private Score |\n",
    "|---|---|---|---|\n",
    "| XGB | 0.8984 | 0.730158730 | 0.738461538 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score :  0.8984212076215782\n"
     ]
    }
   ],
   "source": [
    "def OH(X, hot_cols):\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    for elem in hot_cols:\n",
    "        OH_cols = pd.DataFrame(OH_encoder.fit_transform(X[elem].values.reshape(-1,1)))\n",
    "        OH_cols.rename(columns=lambda x: elem + str(x), inplace=True)\n",
    "        OH_cols.index = X.index\n",
    "        X = pd.concat([X, OH_cols], axis=1)\n",
    "    return X\n",
    "\n",
    "drop_cols_diff = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\", \"ProductCategory\"]\n",
    "\n",
    "pipelineXGB_diff= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols_diff)),\n",
    "    # (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier())\n",
    "])\n",
    "\n",
    "if (True):\n",
    "\n",
    "    print(\"cross validation score : \", cross_val_score(pipelineXGB_diff, OH(train.copy(), hot_cols), train_Y, cv=5, scoring=\"f1\").mean())\n",
    "\n",
    "    pipelineXGB_diff.fit(OH(train.copy(), hot_cols), train_Y)\n",
    "    test_res = pipelineXGB_diff.predict(OH(test.copy(), hot_cols))\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output[\"TransactionId\"] = test[\"TransactionId\"]\n",
    "    output[\"FraudResult\"] = test_res\n",
    "\n",
    "    #save the result to csv file\n",
    "    output.to_csv(\"submission\"+\"diff_CV_zindi\"+\".csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles\n",
    "Nous entrainons les données adaptées sur trois modèles : un Random Forest, un XGBoost et un MLP ; le tableau suivant montre les performances de ces modèles, sans adaptation particulière de leurs paramètres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour le modèle de base\n",
    "\n",
    "drop_cols = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\"]\n",
    "hot_cols = [\"ProductCategory\"]\n",
    "\n",
    "\n",
    "pipelineRF = imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipelineXGB= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier())\n",
    "])\n",
    "\n",
    "pipelineMLP= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", MLPClassifier())\n",
    "])\n",
    "\n",
    "if (False):\n",
    "    for pipeline, name in zip([pipelineRF, pipelineXGB, pipelineMLP],[\"pipelineRF\", \"pipelineXGB\", \"pipelineMLP\"]):\n",
    "        # print(\"cross validation score : \", cross_val_score(pipeline, train, train_Y, cv=5, scoring=\"f1\").mean())\n",
    "        copy_train = train.copy()\n",
    "        copy_train_Y = train_Y.copy()\n",
    "        pipeline.fit(copy_train, copy_train_Y)\n",
    "        test_res = pipeline.predict(test.copy())\n",
    "\n",
    "        output = pd.DataFrame()\n",
    "        output[\"TransactionId\"] = test[\"TransactionId\"]\n",
    "        output[\"FraudResult\"] = test_res\n",
    "\n",
    "        #save the result to csv file\n",
    "        output.to_csv(\"submission\"+name+\".csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont les suivants :\n",
    "\n",
    "| Modèle | Public score | Private score |\n",
    "| --- | --- | --- |\n",
    "| Random Forest | 0.561403508 | 0.517241379 |\n",
    "| XGBoost | 0.741935483 | 0.744186046 |\n",
    "| MLP | 0.070323488 | 0.070412999 |\n",
    "\n",
    "![Random Forest Result](./RFinitialScore.png)\n",
    "![XGBoost Result](./XGBoostinitialScore.png)\n",
    "![MLP Result](./MLPinitialScore.png)\n",
    "\n",
    "Nous voyons clairement que le XGBoost présente les meilleurs résultats. Nous nous tentons donc de partir de cette base pour améliorer encore le modèle. Nous travaillons principalement sur les hyperparamètres suivants  :\n",
    "-\tn_estimators\n",
    "-\tlearning rate\n",
    "-\tmax_depth\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation des hyperparamètres\n",
    "\n",
    "L'optimisation des hyperparamètres des modèles permettrait d'améliorer les performances de ceux-ci. Nous avons utilisé la fonction \"GridSearchCV\" qui prend en paramètres un dictionnaire contenant plusieurs valeurs pour les paramètres du pipeline et qui sélectionne la meilleure combinaison parmi toutes celles possibles. Ce code a été lancé plusieurs fois pour chacun des modèles en utilisant a chaque fois de paramètres de plus en plus précis afin de trouver les meilleurs paramètres possibles (run 1 :n-estimators = [50,100,500,700],... run 2 :n-estimators = [400,500,600],... run 3 :n-estimators = [550,600,650],...).\n",
    "\n",
    "Le fonctionnement de cette fonction nous oblige a sortir le oneHot encoding du pipeline, car elle sépare elle-même le dataset pour faire de la crossvalidation, cela peut mener à des problèmes ou un subSet de donnée ne contient pas le même nombre de colonnes, car une de valeurs possible pour une colonne oneHot encodées n'est pas présente dans celui-ci. Encodée tout le data set avant de l'envoyé dans le pipeline permet donc d'éviter ce problème.\n",
    "\n",
    "Les modèles utilisant les paramètres optimisés donnent de moins bons scores privé et public sur le site du challenge, cela vient de l'estimation de la performance du modèle qui n'est pas correct, et qui empêche donc de converger vers des paramètres corrects.\n",
    "\n",
    "| Modèle | Public Score | Private Score |\n",
    "|---|---|---|\n",
    "| XGB opti | 0.700000000 | 0.704000000 |\n",
    "| XGB non-opti | 0.754098360 | 0.749999999 |\n",
    "\n",
    "\n",
    "Nous avons donc décidé de garder le modèle non optimisé, en sachant qu'il est possible de l'améliorer avec de meilleurs hyperparamètres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OH(X, hot_cols):\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    for elem in hot_cols:\n",
    "        OH_cols = pd.DataFrame(OH_encoder.fit_transform(X[elem].values.reshape(-1,1)))\n",
    "        OH_cols.rename(columns=lambda x: elem + str(x), inplace=True)\n",
    "        OH_cols.index = X.index\n",
    "        X = pd.concat([X, OH_cols], axis=1)\n",
    "    return X\n",
    "\n",
    "drop_cols_opti = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\", \"ProductCategory\"]\n",
    "\n",
    "pipelineXGB_opti= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols_opti)),\n",
    "    # (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "#selection des paramètres par itération manuelle de ce bout de code pour reduira la fourchette a chaque fois sans lancer un code qui tourne pendant 20h\n",
    "param_grid = {'model__n_estimators': [550,600,650,700,800], 'model__max_depth': [2,3,4,6], 'model__learning_rate': [ 0.5,0.55,0.575, 0.6]}\n",
    "\n",
    "if (False):\n",
    "    grid_search = GridSearchCV(pipelineXGB_opti, param_grid, scoring='f1')\n",
    "    grid_search.fit(OH(train, hot_cols), train_Y)\n",
    "\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "# Code pour le modèle avec paramètres améliorés\n",
    "pipelineXGB= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier(learning_rate = 0.5, max_depth = 4, n_estimators = 600))\n",
    "])\n",
    "\n",
    "if (False):\n",
    "    copy_train = train.copy()\n",
    "    copy_train_Y = train_Y.copy()\n",
    "    pipelineXGB.fit(copy_train, copy_train_Y)\n",
    "    test_res = pipelineXGB.predict(test.copy())\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output[\"TransactionId\"] = test[\"TransactionId\"]\n",
    "    output[\"FraudResult\"] = test_res\n",
    "\n",
    "    #save the result to csv file\n",
    "    output.to_csv(\"submission_xgb_opti.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation du pipeline\n",
    "\n",
    "Plusieurs modifications au pipeline ont été essayées:\n",
    "-   Modifier le ratio final de fraude/pas fraude => meilleur résultat avec 25% de fraudes.\n",
    "-   Modification des features droppées ou One Hot encodée => plusieurs variantes ont été testée, le meilleurs résultat actuel est optenu avec : drop_cols_opti = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\", \"ProductCategory\"] et hot_cols = [\"ProductCategory\"]\n",
    "-   Ajout binning de la période de la journée à la place de l'heure => reduit les performances du modèle\n",
    "-   Transformation de la date en jour de la semaine => améliore les performances du modèle\n",
    "-   Ajout d'une colonne \"total\" qui contient la somme des transactions pour chaque client précédant la date de la transaction actuelle => Déteriore grandement les performances du modèle.\n",
    "\n",
    "\n",
    "|  | Public score | Private score |\n",
    "| --- | --- | --- |\n",
    "| Best score (smote 0.25)| 0.754098360 | 0.749999999 |\n",
    "| smote 0.5 | 0.741935483 | 0.744186046  |\n",
    "| smote 0.25 binning | 0.741935483 | 0.702290076 |\n",
    "| smote 0.25 sans weekDayTransformer | 0.657142857 | 0.680851063 |\n",
    "| smote 0.25 avec total | 0.584615384 | 0.610687022 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour le modèle final\n",
    "smt = SMOTE(sampling_strategy=0.5)\n",
    "\n",
    "drop_cols_opti = [\"CurrencyCode\", \"BatchId\", \"CountryCode\", \"CustomerId\", \"PricingStrategy\", \"Amount\", \"ProductCategory\"]\n",
    "hot_cols = [\"ProductCategory\"]\n",
    "\n",
    "pipelineXGB= imbPipeline(steps = [\n",
    "    (\"clean\", StringCleanTransformer()),\n",
    "    (\"amout to sign\", SignTransformer()),\n",
    "    (\"day_time_separator\", DayTimeTransformer()),\n",
    "    (\"Dropper\", DropperTransformer(drop_cols)),\n",
    "    (\"One hot encoding\", OHTransformer(hot_cols)),\n",
    "    (\"weekday\", weekdayTransformer()),\n",
    "    (\"float\", FloatTransformer()),\n",
    "    (\"smote\", smt),\n",
    "    (\"model\", XGBClassifier(n_estimators = 500))\n",
    "])\n",
    "\n",
    "\n",
    "copy_train = train.copy()\n",
    "copy_train_Y = train_Y.copy()\n",
    "pipelineXGB.fit(copy_train, copy_train_Y)\n",
    "test_res = pipelineXGB.predict(test.copy())\n",
    "\n",
    "output = pd.DataFrame()\n",
    "output[\"TransactionId\"] = test[\"TransactionId\"]\n",
    "output[\"FraudResult\"] = test_res\n",
    "\n",
    "#save the result to csv file\n",
    "output.to_csv(\"submission_xgb_0.25.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion et perspectives\n",
    "\n",
    "Le meilleur score obtenu est le suivant: \n",
    "\n",
    "![Best Result](./bestScore.png)\n",
    "\n",
    "Le score obtenu nous semble satisfaisant, d'autant plus qu'il nous place parmi les 128 meilleurs scores du concours, mais nous pensons qu’il est possible de l’améliorer encore. Nous avons plusieurs pistes pour y parvenir qui nous semblent intéressantes à explorer:\n",
    "-   Faire du binning sur l'heure. Le binning testé n'a été testé qu’avec 7 \"bin\" et nous n'avons pas essayé d'associer cette feature à d'autres.\n",
    "-   Modifier le calcul du total de transaction afin de calculer le total sur une période définie et/ou ajouter d'autres paramètres au calcul du total (batchId, channelId,...)\n",
    "-   Testé d'autres ratios de fraudes/pas fraudes pour le SMOTE et expérimenter avec d'autre méthode d'équilibrage (entrainement de plusieurs modèles sans faire d'under/oversampling en utilisant des subset créer a partir de toutes les fraudes et autan de \"pas fraudes\" sélectionnés au hasard et ensuite faire la moyenne de ces modèles...)\n",
    "-   Optimiser les hyperparamètres, cela nécessite cependant de résoudre le problème d'estimation de performances du modèle.\n",
    "-   Faire davantage de feature engineering avec davantage de domaine knowledge. Au regard des résultats obtenus par les participants au concours, il doit y avoir une manipulation de données qui permet d'améliorer nettement le score, mais à ce stade-ci nous manquons probablement de connaissances spécifiques pour la trouver.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
